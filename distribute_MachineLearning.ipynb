{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.5.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 0, 
  "cells": [
    {
      "source": [
        "# Learning a model"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "hide": true
      }
    }, 
    {
      "source": [
        "Installing python 3.5 on a 2.7 Anaconda:\n", 
        "\n", 
        "```\n", 
        "/anaconda/bin/conda create -n py35 python=3.5 anaconda\n", 
        "source /anaconda/envs/py35/bin/activate py35\n", 
        "cd DIR\n", 
        "ipython notebook .\n", 
        "```"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 1, 
      "cell_type": "code", 
      "source": [
        "!pip install seaborn"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 2, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "pd.set_option('display.width', 500)\n", 
        "pd.set_option('display.max_columns', 100)\n", 
        "pd.set_option('display.notebook_repr_html', True)\n", 
        "import seaborn as sns\n", 
        "sns.set_style(\"whitegrid\")\n", 
        "sns.set_context(\"poster\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "execution_count": 3, 
      "cell_type": "code", 
      "source": [
        "import warnings\n", 
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "def make_simple_plot():\n", 
        "    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n", 
        "    axes[0].set_ylabel(\"$y$\")\n", 
        "    axes[0].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_yticklabels([])\n", 
        "    axes[0].set_ylim([-2,2])\n", 
        "    axes[1].set_ylim([-2,2])\n", 
        "    plt.tight_layout();\n", 
        "    return axes\n", 
        "def make_plot():\n", 
        "    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n", 
        "    axes[0].set_ylabel(\"$p_R$\")\n", 
        "    axes[0].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_yticklabels([])\n", 
        "    axes[0].set_ylim([0,1])\n", 
        "    axes[1].set_ylim([0,1])\n", 
        "    axes[0].set_xlim([0,1])\n", 
        "    axes[1].set_xlim([0,1])\n", 
        "    plt.tight_layout();\n", 
        "    return axes"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "source": [
        "## The process of learning"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Some challenges occur in learning such a model from data. These challenges are small samples of data and noise in the data, as well as issues related to the complexity of the models we use, such as bias, and variance or overfitting. We'll encounter hypothesis spaces, and the basic idea of error or risk minimization that is used to learn models. We'll also see the critcal and multifarious role that sampling plays in the learning process.\n", 
        "\n", 
        "Lets say we are trying to predict is a human process such as an election. Here economic and sociological factors are important, such as poverty, race and religiousness. There are historical correlations between such factors and election outcomes which we might want to incorporate into our model. An example of such a model might be:\n", 
        "\n", 
        "*The odds of Romney winning are a function of population religiosity, race, poverty, education, and other social and economic indicators. *\n", 
        "\n", 
        "Our **causal** argument motivating this model here might be that religious people are more socially conservative and thus more likely to vote republican. This might not be the correct causation, but thats not entirely important for the prediction. As long as a **correlation** exists, our model is more structured than 50-50 randomness, and we can try and make a prediction. Remember of-course, our model may even be wrong (see Box's aphorism: https://en.wikipedia.org/wiki/All_models_are_wrong).\n", 
        "\n", 
        "We'll represent the variable being predicted, such as the probability of voting for Romney, by the letter $y$, and the **features** or **co-variates** we use as an input in this probability by the letter $x$. This $x$ could be multi-dimensional, with $x_1$ being poverty, $x_2$ being race, and so on.\n", 
        "\n", 
        "We then write \n", 
        "\n", 
        "$$ y = f(x) $$\n", 
        "\n", 
        "and our jobs is to take $x$ such as data from the census about race, religiousness, and so on, and $y$ as previous elections and the results of polls that pollsters come up with, and to make a predictive model for the elections. That is, we wish to estimate $f(x)$.\n", 
        "\n", 
        "The more domain knowledge we can bring to bear on this problem, the better. The work of folks like Nate Silver isnt just the statistical analysis. They combine their statistical expertise with modelling choices about causal and correlational effects based on their domain knowledge. Sometimes they are right and sometimes wrong."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### A real simple model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To gently step feet in the modelling world, lets see consider very simple model, where the probability of voting for Romney is a function only of how religious the population in a county is."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let $x$ be the fraction of religious people in a county and $y$ be the probability of voting for Romney as a function of $x$. In other words $y_i$ is data that pollsters have taken which tells us their estimate of people voting for Romney and $x_i$ is the fraction of religious people in county $i$. Because poll samples are finite, there is a margin of error on each data point or county $i$, but we will ignore that for now."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let us assume that we have a \"population\" of 200 counties $x$:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "df=pd.read_csv(\"data/religion.csv\")\n", 
        "df.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets suppose now that the Lord came by and told us that the points in the plot below captures $f(x)$ exactly. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 6, 
      "cell_type": "code", 
      "source": [
        "x=df.rfrac.values\n", 
        "f=df.promney.values\n", 
        "plt.plot(x,f,'.')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Notice that our sampling of $x$ is not quite uniform: there are more points around $x$ of 0.7.\n", 
        "\n", 
        "Now, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample $\\cal{D}$ of 30 data points. Such data is called **in-sample data**. Contrastingly, the entire population of data points is also called **out-of-sample data**."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 7, 
      "cell_type": "code", 
      "source": [
        "indexes=np.sort(np.random.choice(x.shape[0], size=30, replace=False))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "samplex = x[indexes]\n", 
        "samplef = f[indexes]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 9, 
      "cell_type": "code", 
      "source": [
        "axes=make_plot()\n", 
        "axes[0].plot(x,f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\n", 
        "axes[0].plot(x,f, 'r.', alpha=0.2, label=\"population\");\n", 
        "axes[1].plot(samplex,samplef, 's', alpha=0.6, label=\"in-sample data $\\cal{D}$\");\n", 
        "axes[0].legend(loc=4);\n", 
        "axes[1].legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "w"
      }
    }, 
    {
      "source": [
        "The lightly shaded squares in the right panel plot are the in-sample $\\cal{D}$ of 30 points given to us. Let us then pretend that we have forgotten the curve that the Lord gave us. Thus, all we know is what is on the plot on the right, and we have no clue about what the original curve was, nor do we remember the original \"population\"."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "That is, imagine the Lord gave us $f$ but then also gave us amnesia. Remember that such amnesia is the general case in learning, where we *do not know* the target function, but rather just have some data. Thus what we will be doing is *trying to find functions that might have generated the 30 points of data that we can see* in the hope that one of these functions might approximate $f$ well, and provide us a **predictive model** for future data. This is known as **fitting** the data."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The Hypothesis or Model Space"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Such a function, one that we use to fit the data, is called a **hypothesis**. We'll use the notation $h$ to denote a hypothesis. Lets consider as hypotheses for the data above, a particular class of functions called polynomials. \n", 
        "\n", 
        "A polynomial is a function that combines multiple powers of x linearly.  You've probably seen these in school, when working with quadratic or cubic equations and functions:\n", 
        "\n", 
        "\\begin{align*}\n", 
        "h(x) &=& 9x - 7 && \\,(straight\\, line) \\\\\n", 
        "h(x) &=& 4x^2 + 3x + 2 && \\,(quadratic) \\\\\n", 
        "h(x) &=& 5x^3 - 31x^2 + 3x  && \\,(cubic).\n", 
        "\\end{align*}\n", 
        "\n", 
        "In general, a polynomial can be written thus:\n", 
        "\n", 
        "\\begin{eqnarray*}\n", 
        " h(x) &=& a_0 + a_1 x^1 + a_2 x^2 + ... + a_n x^n \\\\\n", 
        "      &=& \\sum_{i=0}^{n} a_i x^i\n", 
        "\\end{eqnarray*}\n", 
        "\n", 
        "Thus, by linearly we mean a sum of coefficients $a_i$ times powers of $x$, $x^i$. In other words, the polynomial is **linear in its coefficients**.\n", 
        "\n", 
        "Let us consider as the function we used to fit the data, a hypothesis $h$ that is a straight line. We put the subscript $1$ on the $h$ to indicate that we are fitting the data with a polynomial of order 1, or a straight line. This looks like:\n", 
        "\n", 
        "$$ h_1(x) = a_0 + a_1 x $$\n", 
        "\n", 
        "We'll call the **best fit** straight line the function $g_1(x)$. The \"best fit\" idea is this: amongst the set of all lines (i.e., all possible choices of $h_1(x)$), what is the best line $g_1(x)$ that represents the in-sample data we have? (The subscript $1$ on $g$ is chosen to indicate the best fit polynomial of degree 1, ie the line amongst lines that fits the data best).\n", 
        "\n", 
        "The best fit $g_1(x)$ is calculated and shown in the figure below:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 10, 
      "cell_type": "code", 
      "source": [
        "g1 = np.poly1d(np.polyfit(x[indexes],f[indexes],1))\n", 
        "plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\n", 
        "plt.plot(x,g1(x), 'b--', alpha=0.6, label=\"$g_1$\");\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "How did we calculate the best fit? We'll come to that in a bit, but in the meanwhile, lets formalize and generalize the notion of \"best fit line amongst lines\" a bit.\n", 
        "\n", 
        "The set of all functions of a particular kind that we could have used to fit the data is called a **Hypothesis Space**. The words \"particular kind\" are deliberately vague: its our choice as to what we might want to put into a hypothesis space. A hypothesis space is denoted by the notation $\\cal{H}$.\n", 
        "\n", 
        "Lets consider the hypothesis space of all straight lines $h_1(x)$. We'll denote it as $\\cal{H}_1$, with the subscript being used to mark the order of the polynomial. Another such space might be $\\cal{H}_2$, the hypothesis space of all quadratic functions. A third such space might combine both of these together. We get to choose what we want to put into our hypothesis space.\n", 
        "\n", 
        "In this set-up, what we have done in the code and plot above is this: we have found the best $g_1$ to the data $\\cal{D}$ from the functions in the hypothesis space $\\cal{H}_1$. This is not the best fit from all possible functions, but rather, the best fit from the set of all the straight lines. \n", 
        "\n", 
        "The hypothesis space is a concept we use to capture the **complexity** of a model you use to fit data. For example, since quadratics are more complex functions than straight lines (they curve more), $\\cal{H}_2$ is more complex than $\\cal{H}_1$. \n", 
        "\n", 
        "In this case, suffering from amnesia about the real model, we decided to use the simplest hypothesis space that gives us some indication of the *trend* of our data: the set of straight lines."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Deterministic Error or Bias"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Notice from the figure above that models in $\\cal{H}_1$, i.e., straight lines, and the best-fit straight line $g_1$ in particular, do not do a very good job of capturing the curve of  the data (and thus the underlying function $f$ that we are trying to approximate. Consider the more general case in the figure below, where a curvy $f$ is approximated by a function $g$ which just does not have the wiggling that $f$ has. \n", 
        "\n", 
        "![m:Bias](./images/bias.png)\n", 
        "\n", 
        "There is always going to be an error then, in approximating $f$ by $g$. This *approximation error* is shown in the figure by the blue shaded region, and its called **bias**, or **deterministic error**. The former name comes from the fact that $g$ just does not wiggle the way $f$ does (nothing will make a straight line curve). The latter name (which I first saw used in http://www.amlbook.com/ ) comes from the notion that if you did not know the target function $f$, which is the case in most learning situations, you would have a hard time distinguishing this error from any other errors such as measurement and noise..."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Going back to our model at hand, it is clear that the space of straight lines $\\cal{H}_1$ does not capture the curving in the data. So let us consider the more complex hypothesis space $\\cal{H}_{20}$, the set of all 20th order polynomials $h_{20}(x)$:\n", 
        "\n", 
        "$$h_{20}(x) = \\sum_{i=0}^{20} a_i x^i\\,.$$\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To see how a more complex hypothesis space does, lets find the best fit 20th order polynomial $g_{20}(x)$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 11, 
      "cell_type": "code", 
      "source": [
        "g20 = np.poly1d(np.polyfit(x[indexes],f[indexes],20))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 12, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\n", 
        "plt.plot(x,g20(x), 'b--', alpha=0.6, label=\"$g_{10}$\");\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "Voila! You can see the 20th order polynomial does a much better job of tracking the points, because of the wiggle room it has in making a curve \"go near or through\" all the points as opposed to a straight line, which well, cant curve. Thus it would seem that $\\cal{H}_{10}$ might be a better candidate hypothesis set from which to choose a best fit model. \n", 
        "\n", 
        "We can quantify this by calculating some notion of the bias for both $g_1$ and $g_{20}$. \n", 
        "To do this we calculate the square of the difference between f and the g's on the population of 200 points i.e.:\n", 
        "\n", 
        "$$B_1(x) = (g_1(x) - f(x))^2 \\,;\\,\\, B_{20}(x) = (g_{20}(x) - f(x))^2\\,.$$ \n", 
        "\n", 
        "Squaring makes sure that we are calculating a positive quantity."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 13, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x, (g1(x)-f)**2, lw=3, label=\"$B_1(x)$\")\n", 
        "plt.plot(x, (g20(x)-f)**2, lw=3,label=\"$B_{20}(x)$\");\n", 
        "plt.xlabel(\"$x$\")\n", 
        "plt.ylabel(\"population error\")\n", 
        "plt.yscale(\"log\")\n", 
        "plt.legend(loc=4);\n", 
        "plt.title(\"Bias\");"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "As you can see the **bias or approximation error** is much smaller for $g_{20}$.\n", 
        "\n", 
        "Is $g_{20}$ the best model for this data from all possible models? Indeed, how do we find the best fit model from the best hypothesis space? This is what **learning** is all about.\n", 
        "\n", 
        "We have used the python function `np.polyfit` to find $g_{1}$ the best fit model in $\\cal{H}_{1}$ and $g_{20}$ the best fit model in $\\cal{H}_{20}$, but how did we arrive at that conclusion? This is the subject of the next section. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### How to learn the best fit model in a hypothesis space"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's understand in an intuitive sense, what it means for a function to be a good fit to the data. Lets consider, for now, only the hypothesis space $\\cal{H}_{1}$, the set of all straight lines. In the figure below, we draw against the data points (in red) one such line $h_1(x)$ (in red).\n", 
        "\n", 
        "![m:Cost](./images/linreg.png)\n", 
        "\n", 
        "The natural way of thinking about a \"best fit\" would be to minimize the distance from the line to the points, for some notion of distance. In the diagram we depict one such notion of distance: the vertical distance from the points to the line. These distances are represented as thin black lines.\n", 
        "\n", 
        "The next question that then arises is this: how exactly we define the measure of this vertical distance? We cant take the measure of distance to be the y-value of the point minus the y value of the line at the same x, ie $y_i - h_1(x_i)$. Why? If we did this, then we could have points very far from the line, and as long as the total distance above was equal to the total distance below the line, we'd get a net distance of 0 even when the line is very far from the points.\n", 
        "\n", 
        "Thus we must use a positive estimate of the distance as our measure. We could take either the absolute value of the distance, $\\vert y_i - h_1(x_i) \\vert$, or the square of the distance as our measure, $(y_i - h_1(x_i))^2$. Both are reasonable choices, and we shall use the squared distance for now. (Now its probably clear to you why we defined bias in the last section as the pointwise square of the distance).\n", 
        "\n", 
        "We sum this measure up over all our data points, to create whats known as the **error functional** or **risk functional** (also just called **error*, **cost**, or **risk**) of using line $h_1(x)$ to fit our points $y_i \\in \\cal{D}$ (this notation is to be read as \"$y_i$ in $\\cal{D}$\") :\n", 
        "\n", 
        "$$ R_{\\cal{D}}(h_i(x)) = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} (y_i - h_1(x_i))^2 $$\n", 
        "\n", 
        "where $N$ is the number of points in $\\cal{D}$.\n", 
        "\n", 
        "What this formula says is: the cost or risk is just the total squared distance to the line from the observation points. Here we use the word **functional** to denote that, just as in functional programming, the risk is a *function of the function* $h_1(x)$. \n", 
        "\n", 
        "We also make explicit the in-sample data $\\cal{D}$, because the value of the risk depends upon the points at which we made our observation. If we had made these observations $y_i$ at a different set of $x_i$, the value of the risk would be somewhat different. The hope in learning is that the risk will not be too different, as we shall see in the next section\n", 
        "\n", 
        "Now, given these observations, and the hypothesis space $\\cal{H}_1$, we minimize the risk over all possible functions in the hypothesis space to find the **best fit** function $g_1(x)$:\n", 
        "\n", 
        "$$ g_1(x) = \\arg\\min_{h_1(x) \\in \\cal{H}} R_{\\cal{D}}(h_1(x)).$$\n", 
        "\n", 
        "Here the notation \n", 
        "\n", 
        "$\"\\arg\\min_{x} F(x)\"$ \n", 
        "\n", 
        "means: give me the argument of the functional $x$ at which $F(x)$ is minmized. So, for us: give me the function $g_1(x) = h_1$ at which the risk $R_{\\cal{D}}(h_1)$ is minimized; i.e. the minimization is over *functions* $h_1$.\n", 
        "\n", 
        "(We'll wont worry about how to actually do this minimization, since the `sklearn` and `statsmodels` functions actually do this for us, currently we are just interested in the conceptual notions).\n", 
        "\n", 
        "And this is exactly what the python function `np.polyfit(x,h,n)` does for us. It minimizes this squared-error with respect to the coefficients of the polynomial.\n", 
        "\n", 
        "Thus we can in general write:\n", 
        "\n", 
        "$$ g(x) = \\arg\\min_{h(x) \\in \\cal{H}} R_{\\cal{D}}(h(x)),$$\n", 
        "\n", 
        "where $\\cal{H}$ is a general hypothesis space of functions."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## The Structure of Learning"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We have a target function $f(x)$ that we do not know. But we do have a sample of data points from it, $(x_1,y_1), (x_2,y_2), ..., (x_n,y_n)$. We call this the **sample** or **training examples** $\\cal{D}$. We are interested in using this sample to estimate a function $g$ to approximate the function $f$, and which can be used for prediction at new data points, or on the entire population, also called **out-of-sample prediction**. \n", 
        "\n", 
        "To do this, we use an algorithm, called the **learner**, which chooses functions from a hypothesis set $\\cal{H}$ and computes a cost measure or risk functional $R$ (like the sum of the squared distance over all points in the data set) for each of these functions. It then chooses the function $g$ which **minimizes** this cost measure amonst all the functions in $\\cal{H}$, and gives us a final hypothesis $g$ which we then use to approximate or estimate f **everywhere**, not just at the points in our data set. \n", 
        "\n", 
        "Here our learner is called **Polynomial Regression**, and it takes a hypothesis space $\\cal{H}_d$ of degree $d$ polynomials, minimizes the \"squared-error\" risk measure, and spits out a best-fit hypothesis $g_d$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Empirical Risk Minimization"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We write $g \\approx f$, or $g$ is the **estimand** of $f$. This process is called **Empirical Risk Minimization** (ERM) as we minimize the cost measure over the \"empirically observed\" training examples or points.\n", 
        "\n", 
        "Why do we think that this might be a good idea? What are we really after?\n", 
        "\n", 
        "What we'd like to do is **make good predictions**. In the language of cost, what we are really after is to minimize the cost **out-of-sample**, on the population at large. But this presents us with a conundrum: **how can we minimize the risk on points we havent yet seen**?\n", 
        "\n", 
        "This is why we (a) minimize the risk on the set of points that we have, doing ERM to find $g$ and then (b) hope that once we have found our best model $g$, our risk does not particularly change out-of-sample, or when using a different set of points\n", 
        "\n", 
        "In a sense, this is our old issue (like in HW2) : we are once again drawing conclusions about a population from a sample. (The only difference here is that we have features $x_i$ as well as data points $y_i$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The role of sampling"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Intuitively, to do this, we need to ask ourselves, how representative is our sample? Or more precisely, how representative is our sample of our training points of the population (or for that matter the new x that we want to predict for)? \n", 
        "\n", 
        "We illustrate this below for our population of 200 data points and our sample of 30 data points (in red)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 14, 
      "cell_type": "code", 
      "source": [
        "plt.hist(x, normed=True, bins=30, alpha=0.7)\n", 
        "sns.kdeplot(x)\n", 
        "plt.plot(x[indexes], [1.0]*len(indexes),'o', alpha=0.8)\n", 
        "plt.xlim([0,1]);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "In our example, if we only want to use $g$, our estimand of $f$ to predict for large $x$, or more religious counties, we would need a good sampling of points $x$ closer to 1. And, similarly, the new $x$ we are using to make predictions would also need to be representative of those counties. We wont do well if we try and predict low-religiousness counties from a sample of high-religiousness ones. Or, if we do want to predict over the entire range of religiousness, our training sample better cover all $x$ well.\n", 
        "\n", 
        "Our red points seem to follow our (god given) histogram well."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Statement of the learning problem."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Once we have done that, we can then intuitively say that, if we find a hypothesis $g$ that minimizes the cost or risk over the training set; this hypothesis *might* do a good job over the population that the training set was representative of, since the risk on the population ought to be similar to that on the training set, and thus small.\n", 
        "\n", 
        "Mathematically, we are saying that:\n", 
        "\n", 
        "\\begin{eqnarray*}\n", 
        "A &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\n", 
        "B &:& R_{out \\,of \\,sample} (g) \\approx R_{\\cal{D}}(g)\n", 
        "\\end{eqnarray*}\n", 
        "\n", 
        "\n", 
        "In other words, we hope the **empirical risk estimates the out of sample risk well, and thus the out of sample risk is also small**."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Indeed, as we can see below, $g_{20}$ does an excellent job on the population, not just on the sample."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 15, 
      "cell_type": "code", 
      "source": [
        "#plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\n", 
        "plt.plot(x,g20(x), 'b--', alpha=0.9, lw=2, label=\"$g_{20}$\");\n", 
        "plt.plot(x,f, 'o', alpha=0.2, label=\"population\");\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "## Noise"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "You probably noticed that I used weasel words like \"might\" and \"hope\" in the last section when saying that representative sampling in both training and test sets combined with ERM is what we need to learn a model. Let me give you a very simple counterexample: a prefect memorizer.\n", 
        "\n", 
        "Suppose I construct a model which memorizes all the data points in the training set. Then its emprical risk is zero by definition, but it has no way of predicting anything on a test set. Thus it might as well choose the value at a new point randomly, and will perform very poorly. The process of interpolating a curve from points is precisely this memorization"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We saw in the diagram above that $g_{20}$ did a very good job in capturing the curves of the population. However, note that the data obtained from $f$, our target, was still quite smooth. Most real-world data sets are not smooth at all, because of various effects such as measurement errors, other co-variates, and so on. Such **stochastic noise** plays havoc with our fits, as we shall see soon."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Stochastic Noise, or the problem with randomness."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Stochastic noise bedevils almost every data set known to humans, and happens for many different reasons. \n", 
        "\n", 
        "Consider for example two customers of a bank with identical credit histories and salaries. One defaults on their mortgage, and the other does not. In this case we have identical $x = (credit, salary)$ for these two customers, but different $y$, which is a variable that is 1 if the customer defaulted and 0 otherwise. The true $y$ here might be a function of other co-variates, such as marital strife, sickness of parents, etc. But, as the bank, we might not have this information. So we get different $y$ for different customers at the information $x$ that we possess.\n", 
        "\n", 
        "A similar thing might be happen in the election example, where we have modelled the probability of voting for romney as a function of religiousness of the county. There are many other variables we might not have measured, such as the majority race in that county.  But, we have not measured this information. Thus, in counties with high religiousness fraction $x$ we might have more noise than in others. Consider for example two counties, one with $x=0.8$ fraction of self-identified religious people in the county, and another with $x=0.82$. Based on historical trends, if the first county was mostly white, the fraction of those claiming they would vote for Romney might be larger than in a second, mostly black county. Thus you might have two very $y$'s next to each other on our graphs.\n", 
        "\n", 
        "It gets worse. When pollsters estimate the number of people voting for a particular candidate, they only use finite samples of voters. So there is a 4-6\\% polling error in any of these estimates. This \"sampling noise\" adds to the noisiness of the $y$'s. \n", 
        "\n", 
        "\n", 
        "Indeed, we wish to estimate a function $f(x)$ so that the values $y_i$ come from the function $f$. Since we are trying to estimate f with data from only some counties, and furthermore, our estimates of the population behaviour in these counties will be noisy, our estimate wont be the \"god given\" or \"real\" f, but rather some **noisy** estimate of it. \n", 
        "\n", 
        "Lets simulate these errors to see how they affect the process of learning."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 16, 
      "cell_type": "code", 
      "source": [
        "sigma=0.06\n", 
        "mask=(x > 0.65) & (x < 0.8)\n", 
        "sigmalist=sigma+mask*0.03"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 17, 
      "cell_type": "code", 
      "source": [
        "y = f + sp.stats.norm.rvs(scale=sigmalist, size=200)\n", 
        "#the next three lines just ensure that y remains a probability\n", 
        "yadd = (y < 0.0) *(0.01-y)\n", 
        "ysub = (y > 1.0)*(y - 1.0)\n", 
        "y = y + yadd -ysub"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 18, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x,f, 'r-', alpha=0.6, label=\"f\");\n", 
        "plt.plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"in-sample y (observed)\");\n", 
        "plt.plot(x, y, '.', alpha=0.6, label=\"population y\");\n", 
        "plt.xlabel('$x$');\n", 
        "plt.ylabel('$p_R$')\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "In the figure above, one can see the scatter of the $y$ population about the curve of $f$. The errors of the 30 observation points (\"in-sample\") are shown as squares. One can see that observations next to each other can now be fairly different, as we descibed above."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Systematic error"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "There is yet another class of errors called **systematic errors**. For example, sampling of respondents for an election might be done poorly. For example, if you were to only call people with land-lines to ask such questions, your respondents would likely skew older. And you would have a sampling bias in your polls. In the 2012 poll cycle, Rasmunnsen had a systematic bias in favor of republicans which had to be corrected for by various forecasters.^[http://fivethirtyeight.blogs.nytimes.com/2012/11/10/which-polls-fared-best-and-worst-in-the-2012-presidential-race/?_r=0].\n", 
        "\n", 
        "Similarly, in observing or experimenting with physical phenomenon, the measuring instruments might have a systematic error. For example, an altimeter needs to be callibrated to an altitude correcly. If you do this wrong, you will have wrong altitudes (by the same amount) everywhere. Also, if the weather changes while you are at that original altitude, you might lose that original callibration (since altimeters rely on air pressure, the altimeter will think you've moved to a different altitude). As another example, if a measuting rope has stretched out, your yardage  measurements on a football field might be different.\n", 
        "\n", 
        "These sorts of errors cannot be modelled statistically and need to be dealt with on a case by case basis. They are not what we are talking about here, but as a modeler, you must be alert to their possibility or prescence."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Fitting a noisy model: the complexity of your hypothesis"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let us now try and fit the noisy data we simulated above, both using straight lines ($\\cal{H}_1$), and 20th order polynomials($\\cal{H}_{20}$). \n", 
        "\n", 
        "What we have done is introduced a noisy target $y$, so that\n", 
        "\n", 
        "$$y = f(x) + \\epsilon\\,,$$\n", 
        "\n", 
        "where $\\epsilon$ is a **random** noise term that represents the stochastic noise. In the simulation of errors above we assumed that $\\epsilon$ is drawn from a bell curve. Note that this means that $\\epsilon$ will be different at different $x$, with its values chosen from a bell curve.\n", 
        "\n", 
        "Another way to think about a noisy $y$ is to imagine that our data is generated from a joint probability distribution $P(x,y)$. In our earlier case with no stochastic noise, once you knew $x$, if I were to give you $f(x)$, you could give me $y$ exactly. This is now not possible because of the noise $\\epsilon$: we dont know exactly how much noise we have at any given $x$. Thus we need to model $y$ at a given $x$, written as $P(y|x)$, as well using a probability distribution (a bell curve in our example). Since $P(x)$ is also a probability distribution, we have:\n", 
        "\n", 
        "$$P(x,y) = P(y | x) P(x) .$$\n", 
        "\n", 
        "Now the entire learning problem can be cast as a problem in probability **density estimation**: if we can estimate $P(x,y)$ and take actions based on that estimate thanks to our risk or error functional, we are done. More on this in the notebook on classification.\n", 
        "\n", 
        "We now fit in both $\\cal{H}_1$ and $\\cal{H}_{20}$ to find the best fit straight line and best fit 20th order polynomial respectively."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 19, 
      "cell_type": "code", 
      "source": [
        "g1noisy = np.poly1d(np.polyfit(x[indexes],y[indexes],1))\n", 
        "g20noisy = np.poly1d(np.polyfit(x[indexes],y[indexes],20))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 20, 
      "cell_type": "code", 
      "source": [
        "axes=make_plot()\n", 
        "axes[0].plot(x,f, 'r-', alpha=0.6, label=\"f\");\n", 
        "axes[1].plot(x,f, 'r-', alpha=0.6, label=\"f\");\n", 
        "axes[0].plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\n", 
        "axes[1].plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\n", 
        "axes[0].plot(x,g1(x),  'b--', alpha=0.6, label=\"$g_1 (no noise)$\");\n", 
        "axes[0].plot(x,g1noisy(x), 'b:', lw=4, alpha=0.8, label=\"$g_1 (noisy)$\");\n", 
        "axes[1].plot(x,g20(x),  'b--', alpha=0.6, label=\"$g_10 (no noise)$\");\n", 
        "axes[1].plot(x,g20noisy(x), 'b:', lw=4, alpha=0.8, label=\"$g_{10}$ (noisy)\");\n", 
        "axes[0].legend(loc=4);\n", 
        "axes[1].legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "w"
      }
    }, 
    {
      "source": [
        "The results are (to put it mildly) very interesting. \n", 
        "\n", 
        "Lets look at the figure on the left first. The noise changes the best fit line by a little but not by much. The best fit line still does a very poor job of capturing the variation in the data.\n", 
        "\n", 
        "The best fit 20th order polynomial, in the presence of noise, is very interesting. It tries to follow all the curves of the observations..in other words, it tries to fit the noise. This is a disaster, as you can see if you plot the population (out-of-sample) points on the plot as well:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 21, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x,f, 'r-', alpha=0.6, label=\"f\");\n", 
        "plt.plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\n", 
        "plt.plot(x,y,  '.', alpha=0.6, label=\"population y\");\n", 
        "plt.plot(x,g20noisy(x), 'b:', alpha=0.6, label=\"$g_{10}$ (noisy)\");\n", 
        "plt.ylim([0,1])\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Whoa. The best-fit 20th order polynomial does a reasonable job fitting the in-sample data, and is even well behaved in the middle where we have a lot of in-sample data points. But at places with less in-sample data points, the polynomial wiggles maniacally.\n", 
        "\n", 
        "This fitting to the noise is a danger you will encounter again and again in learning. Its called **overfitting**. So, $\\cal{H}_{20}$ which seemed to be such a good candidate hypothesis space in the absence of noise, ceases to be one. The take away lesson from this is that we must further ensure that our **model does not fit the noise**.\n", 
        "\n", 
        "Lets make a plot similar to the one we made for the deterministic noise earlier, and compare the error in the new $g_1$ and $g_20$ fits on the noisy data."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 22, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x, ((g1noisy(x)-f)**2), lw=3, label=\"$g_1$\")\n", 
        "plt.plot(x, ((g20noisy(x)-f)**2), lw=3,label=\"$g_{20}$\");\n", 
        "plt.plot(x, [1]*x.shape[0], \"k:\", label=\"noise\", alpha=0.2);\n", 
        "for i in indexes[:-1]:\n", 
        "    plt.axvline(x[i], 0, 1, color='r', alpha=0.1)\n", 
        "plt.axvline(x[indexes[-1]], 0, 1, color='r', alpha=0.1, label=\"samples\")\n", 
        "plt.xlabel(\"$x$\")\n", 
        "plt.ylabel(\"population error\")\n", 
        "plt.yscale(\"log\")\n", 
        "plt.legend(loc=4);\n", 
        "plt.title(\"Noisy Data\");"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "$g_1$ now, for the most part, has a lower error! So you'd be better off by having chosen a set of models with much more bias (the straight lines, $\\cal{H}_1$) than a more complex model set ($\\cal{H}_{20}$) in the case of noisy data. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The Variance of your model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "This tendency of a more complex model to overfit, by having enough freedom to fit the noise, is described by something called high **variance**. What is variance?\n", 
        "\n", 
        "Variance, simply put, is the \"error-bar\" or spread in models that would be learnt by training on different data sets $\\cal{D}_1, \\cal{D}_2,...$ drawn from the population. Now, this seems like a circular concept, as in real-life, you do not have access to the population. But since we simulated our data here anyways, we do, and so let us see what happens if we choose **different 30 points randomly from our population of 200**, and fit models in both $\\cal{H}_1$ and $\\cal{H}_{20}$ to them. We do this on 200 sets of randomly chosen (from the population) data sets of 30 points each and plot the best fit models in noth hypothesis spaces for all 200 sets."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 23, 
      "cell_type": "code", 
      "source": [
        "def gen(degree, nsims, size, x, out):\n", 
        "    outpoly=[]\n", 
        "    for i in range(nsims):\n", 
        "        indexes=np.sort(np.random.choice(x.shape[0], size=size, replace=False))\n", 
        "        pc=np.polyfit(x[indexes], out[indexes], degree)\n", 
        "        p=np.poly1d(pc)\n", 
        "        outpoly.append(p)\n", 
        "    return outpoly"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 24, 
      "cell_type": "code", 
      "source": [
        "polys1 = gen(1, 200, 30,x, y);\n", 
        "polys20 = gen(20, 200, 30,x, y);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 25, 
      "cell_type": "code", 
      "source": [
        "axes=make_plot()\n", 
        "axes[0].plot(x,f, 'r-', lw=3, alpha=0.6, label=\"f\");\n", 
        "axes[1].plot(x,f, 'r-', lw=3, alpha=0.6, label=\"f\");\n", 
        "axes[0].plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"data y\");\n", 
        "axes[1].plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"data y\");\n", 
        "axes[0].plot(x, y, '.', alpha=0.6, label=\"population y\");\n", 
        "axes[1].plot(x, y, '.', alpha=0.6, label=\"population y\");\n", 
        "c=sns.color_palette()[2]\n", 
        "for i,p in enumerate(polys1[:-1]):\n", 
        "    axes[0].plot(x,p(x), alpha=0.05, c=c)\n", 
        "axes[0].plot(x,polys1[-1](x), alpha=0.05, c=c,label=\"$g_1$ from different samples\")\n", 
        "for i,p in enumerate(polys20[:-1]):\n", 
        "    axes[1].plot(x,p(x), alpha=0.05, c=c)\n", 
        "axes[1].plot(x,polys20[-1](x), alpha=0.05, c=c, label=\"$g_{10}$ from different samples\")\n", 
        "axes[0].legend(loc=4);\n", 
        "axes[1].legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "w"
      }
    }, 
    {
      "source": [
        "On the left panel, you see the 200 best fit straight lines, each a fit on a different 30 point training sets from the 200 point population. The best-fit lines bunch together, even if they dont quite capture $f$ (the thick red line) or the data (squares) terribly well.\n", 
        "\n", 
        "On the right panel, we see the same with best-fit models chosen from $\\cal{H}_{20}$. It is a diaster. While most of the models still band around the central trend of the real curve $f$ and data $y$ (and you still see the waves corresponding to all too wiggly 20th order polynomials), a substantial amount of models veer off into all kinds of noisy hair all over the plot. This is **variance**: the the predictions at any given $x$ are all over the place.\n", 
        "\n", 
        "The variance can be seen in a different way by plotting the coefficients of the polynomial fit. Below we plot the coefficients of the fit in $\\cal{H}_1$. The variance is barely 0.2 about the mean for both co-efficients."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 26, 
      "cell_type": "code", 
      "source": [
        "pdict1={}\n", 
        "pdict20={}\n", 
        "for i in reversed(range(2)):\n", 
        "    pdict1[i]=[]\n", 
        "    for j, p in enumerate(polys1):\n", 
        "        pdict1[i].append(p.c[i])\n", 
        "for i in reversed(range(21)):\n", 
        "    pdict20[i]=[]\n", 
        "    for j, p in enumerate(polys20):\n", 
        "        pdict20[i].append(p.c[i]) \n", 
        "df1=pd.DataFrame(pdict1)\n", 
        "df20=pd.DataFrame(pdict20)\n", 
        "fig = plt.figure(figsize=(14, 5)) \n", 
        "from matplotlib import gridspec\n", 
        "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 10]) \n", 
        "axes = [plt.subplot(gs[0]), plt.subplot(gs[1])]\n", 
        "axes[0].set_ylabel(\"value\")\n", 
        "axes[0].set_xlabel(\"coeffs\")\n", 
        "axes[1].set_xlabel(\"coeffs\")\n", 
        "plt.tight_layout();\n", 
        "sns.violinplot(df1, ax=axes[0]);\n", 
        "sns.violinplot(df20, ax=axes[1]);\n", 
        "axes[0].set_yscale(\"symlog\");\n", 
        "axes[1].set_yscale(\"symlog\");\n", 
        "axes[0].set_ylim([-1e12, 1e12]);\n", 
        "axes[1].set_ylim([-1e12, 1e12]);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "w"
      }
    }, 
    {
      "source": [
        "In the right panel we plot the coefficients of the fit in $\\cal{H}_{20}$. This is why we use the word \"variance\": the spread in the values of the middle coefficients about their means (dashed lines) is of the order $10^{10}$ (the vertical height of the bulges), with huge outliers!! The 20th order polynomial fits are a disaster!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "\n", 
        "So far?\n", 
        "\n", 
        "- you have learnt the basic formulation of the learning problem, the concept of a hypothesis space, and a strategy using minimization of distance (called cost or risk) to find the best fit model for the target function from this hypothesis space. \n", 
        "- You learned the effect of noise on this fit, and the issues that crop up in learning target functions from data, chiefly the problem of overfitting to this noise. \n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The process of learning has two parts:\n", 
        "\n", 
        "1. Fit for a model by minimizing the in-sample risk\n", 
        "2. Hope that the in-sample risk approximates the out-of-sample risk well."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Well, we are scientists. Just hoping does not befit us. But we only have a sample. What are we to do?\n", 
        "\n", 
        "The \"aha\" moment comes when we realize that we can hold back some of our sample, and test the performance of our learner by trying it out on this held back part! Perhaps we can compute the error or risk on the held-out part, or \"test\" part of our sample, and have something to say about the out-of-sample error."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Testing and Training Sets"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let us introduce some new terminology. We take the sample of data $\\cal{D}$ that we have been given (our in-sample set) and split it into two parts:\n", 
        "\n", 
        "1. The **training set**, which is the part of the data we use to fit a model\n", 
        "2. The **testing set**, a smaller part of the data set which we use to see how good our fit was.\n", 
        "\n", 
        "This split is done by choosing points at random into these two sets. Typically we might take 80% of our data and put it in the training set, with the remaining amount going into the test set. This can be carried out in python using the `train_test_split` function from `sklearn.cross_validation`.\n", 
        "\n", 
        "The split is shown in the diagram below:\n", 
        "\n", 
        "![m:caption](images/train-test.png)\n", 
        "\n", 
        "We ARE taking a hit on the amount of data we have to train our model. The more data we have, the better we can do for our fits. But, you cannot figure out the generalization ability of a learner by looking at the same data it was trained on: there is nothing to generalize to, and as we know we can fit very complex models to training data which have no hope of generalizing (like an interpolator). Thus, to estimate the **out-of-sample error or risk**, we must leave data over to make this estimation. \n", 
        "\n", 
        "At this point you are thinking: the test set is just another sample of the population, just like the training set. What guarantee do we have that it approximates the out-of-sample error well? And furthermore, if we pick 6 out of 30 points as a test set, why would you expect the estimate to be any good?\n", 
        "\n", 
        "Its not possible to prove it in this course, but the test set error is a good estimate of the out of sample error, especially for larger and larger test sets. You are right to worry that 6 points is perhaps too few, but thats what we have for now, and we shall work with them.\n", 
        "\n", 
        "We are **using the training set then, as our in-sample set, and the test set as a proxy for out-of-sample.**."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 27, 
      "cell_type": "code", 
      "source": [
        "df=pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 28, 
      "cell_type": "code", 
      "source": [
        "from sklearn.cross_validation import train_test_split\n", 
        "datasize=df.shape[0]\n", 
        "#split dataset using the index, as we have x,f, and y that we want to split.\n", 
        "itrain,itest = train_test_split(range(30),train_size=24, test_size=6)\n", 
        "xtrain= df.x[itrain].values\n", 
        "ftrain = df.f[itrain].values\n", 
        "ytrain = df.y[itrain].values\n", 
        "xtest= df.x[itest].values\n", 
        "ftest = df.f[itest].values\n", 
        "ytest = df.y[itest].values"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 29, 
      "cell_type": "code", 
      "source": [
        "axes=make_plot()\n", 
        "axes[0].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\n", 
        "axes[0].plot(df.x,df.y, 'o',alpha=0.6, label=\"$\\cal{D}$\");\n", 
        "axes[1].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\n", 
        "axes[1].plot(xtrain, ytrain, 's', label=\"training\")\n", 
        "axes[1].plot(xtest, ytest, 's', label=\"testing\")\n", 
        "axes[0].legend(loc=\"lower right\")\n", 
        "axes[1].legend(loc=\"lower right\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### A short digression about scikit-learn"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Scikit-learn is the main python machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as `train_test_split`. It can be used in python by the incantation `import sklearn`.\n", 
        "\n", 
        "The library has a very well defined interface. This makes the library a joy to use, and surely contributes to its popularity. As the [scikit-learn API paper](http://arxiv.org/pdf/1309.0238v1.pdf) [Buitinck, Lars, et al. \"API design for machine learning software: experiences from the scikit-learn project.\" arXiv preprint arXiv:1309.0238 (2013).] says:\n", 
        "\n", 
        ">All objects within scikit-learn share a uniform common basic API consisting of three complementary interfaces: **an estimator interface for building and \ufb01tting models, a predictor interface for making predictions and a transformer interface for converting data**. The estimator interface is at the core of the library. It de\ufb01nes instantiation mechanisms of objects and exposes a `fit` method for learning a model from training data. All supervised and unsupervised learning algorithms (e.g., for classi\ufb01cation, regression or clustering) are o\ufb00ered as objects implementing this interface. Machine learning tasks like feature extraction, feature selection or dimensionality reduction are also provided as estimators.\n", 
        "\n", 
        "Earlier we fit `y` using the python function `polyfit`. To get you familiarized with scikit-learn, we'll use the \"estimator\" interface here, specifically the estimator `PolynomialFeatures`. The API paper again:\n", 
        "\n", 
        ">Since it is common to modify or \ufb01lter data before feeding it to a learning algorithm, some estimators in the library implement a transformer interface which de\ufb01nes a transform method. It takes as input some new data X and yields as output a transformed version of X. Preprocessing, feature selection, feature extraction and dimensionality reduction algorithms are all provided as transformers within the library.\n", 
        "\n", 
        "To start with we have one **feature** `x`, the fraction of religious people in a county, which we want to use to predict `y`, the fraction of people voting for Romney in that county. What we will do is the transformation:\n", 
        "\n", 
        "$$ x \\rightarrow 1, x, x^2, x^3, ..., x^d $$\n", 
        "\n", 
        "for some power $d$. Our job then is to **fit** for the coefficients of these features in the polynomial\n", 
        "\n", 
        "$$ a_0 + a_1 x + a_2 x^2 + ... + a_d x^d. $$\n", 
        "\n", 
        "In other words, we have transformed a function of one feature, into a (rather simple) **linear** function of many features. To do this we first construct the estimator as `PolynomialFeatures(d)`, and then transform these features into a d-dimensional space using the method `fit_transform`.\n", 
        "\n", 
        "![fit_transform](images/sklearntrans.jpg)\n", 
        "\n", 
        "Here is an example. The reason for using `[[1],[2],[3]]` as opposed to `[1,2,3]` is that scikit-learn expects data to be stored in a two-dimensional array or matrix with size `[n_samples, n_features]`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 30, 
      "cell_type": "code", 
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n", 
        "PolynomialFeatures(3).fit_transform([[1],[2], [3]])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "To transform `[1,2,3]` into [[1],[2],[3]] we need to do a reshape.\n", 
        "\n", 
        "![reshape](images/reshape.jpg)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "np.array([1,2,3]).reshape(-1,1)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "So now we are in the recatangular, rows=samples, columns=features form expected by `scikit-learn`. Ok, so lets see the process to transform our 1-D dataset `x` into a d-dimensional one. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 32, 
      "cell_type": "code", 
      "source": [
        "xtrain"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 33, 
      "cell_type": "code", 
      "source": [
        "xtrain.reshape(-1,1)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 34, 
      "cell_type": "code", 
      "source": [
        "PolynomialFeatures(2).fit_transform(xtrain.reshape(-1,1))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets put this alltogether. Below we create multiple datasets, one for each polynomial degree:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 35, 
      "cell_type": "code", 
      "source": [
        "def make_features(train_set, test_set, degrees):\n", 
        "    traintestlist=[]\n", 
        "    for d in degrees:\n", 
        "        traintestdict={}\n", 
        "        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n", 
        "        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n", 
        "        traintestlist.append(traintestdict)\n", 
        "    return traintestlist"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "###How do training and testing error change with complexity?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "You will recall that the big question we were left with earlier is: what order of polynomial should we use to fit the data? Which order is too biased? Which one has too much variance and is too complex? Let us try and answer this question.\n", 
        "\n", 
        "We do this by fitting 14 different models (remember the fit is made by minimizing the empirical risk on the training set), each with increasing dimension `d`, and looking at the training-error and the test-error in each of these models. So we first try $\\cal{H}_0$, then $\\cal{H}_1$, then $\\cal{H}_2$, and so on.\n", 
        "\n", 
        "Since we use `PolynomialFeatures` above, each increasing dimension gives us an additional feature. $\\cal{H}_5$ has 6 features, a constant and the 5 powers of `x`. What we want to do is to find the coefficients of the 5-th order polynomial that best fits the data. Since the polynomial is **linear** in the coefficients (we multiply coefficients by powers-of-x features and sum it up), we use a learner called a `LinearRegression` model (remember that the \"linear\" in the regression refers to linearity in co-efficients). The scikit-learn interface to make such a fit is also very simple, the function `fit`. And once we have learned a model, we can predict using the function `predict`. The API paper again:\n", 
        "\n", 
        ">The predictor interface extends the notion of an estimator by adding a predict method that takes an array X_test and produces predictions for X_test, based on the learned parameters of the estimator.\n", 
        "\n", 
        "So, for increasing polynomial degree, and thus feature dimension `d`, we fit a `LinearRegression` model on the traing set. We then use scikit-learn again to calculate the error or risk. We calculate the `mean_squared_error` between the model's predictions and the data, BOTH on the training set and test set. We plot this error as a function of the defree of the polynomial `d`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 36, 
      "cell_type": "code", 
      "source": [
        "from sklearn.linear_model import LinearRegression\n", 
        "from sklearn.metrics import mean_squared_error\n", 
        "\n", 
        "degrees=range(21)\n", 
        "error_train=np.empty(len(degrees))\n", 
        "error_test=np.empty(len(degrees))\n", 
        "\n", 
        "traintestlists=make_features(xtrain, xtest, degrees)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "####The structure of `scikit-learn`\n", 
        "\n", 
        "Once again, lets see the structure of scikit-learn needed to make these fits. `.fit` always takes two arguments:\n", 
        "\n", 
        "`estimator.fit(Xtrain, ytrain)`.\n", 
        "\n", 
        "Here `Xtrain` must be in the form of an array of arrays, with the inner array each corresponding to one sample, and whose elements correspond to the feature values for that sample. (This means that the 4th element for each of these arrays, in our polynomial example, corresponds to the valueof $x^3$ for each \"sample\" $x$). The `ytrain` is a simple array of responses..continuous for regression problems, and categorical values or 1-0's for classification problems.\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "![reshape](images/sklearn2.jpg)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 37, 
      "cell_type": "code", 
      "source": [
        "traintestlists[3]['train'], ytrain"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The test set `Xtest` has the same structure, and is used in the `.predict` interface. Once we have fit the estimator, we predict the results on the test set by:\n", 
        "\n", 
        "`estimator.predict(Xtest)`.\n", 
        "\n", 
        "The results of this are a simple array of predictions, of the same form and shape as `ytest`.\n", 
        "\n", 
        "#### On one test and training set"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 38, 
      "cell_type": "code", 
      "source": [
        "egest = LinearRegression()\n", 
        "egest.fit(traintestlists[3]['train'], ytrain)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 39, 
      "cell_type": "code", 
      "source": [
        "traintestlists[3]['test'], ytest"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 40, 
      "cell_type": "code", 
      "source": [
        "egest.predict(traintestlists[3]['test'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 41, 
      "cell_type": "code", 
      "source": [
        "mean_squared_error(ytest, egest.predict(traintestlists[3]['test']))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We can then use `mean_squared_error` from `sklearn` to calculate the error between the predictions and actual `ytest` values. Below we calculate this error on both the training set (which we already fit on) and the test set (which we hadnt seen before), and plot how these errors change with the degree of the polynomial."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "for d in degrees:#for increasing polynomial degrees 0,1,2...\n", 
        "    Xtrain = traintestlists[d]['train']\n", 
        "    Xtest = traintestlists[d]['test']\n", 
        "    #set up model\n", 
        "    est = LinearRegression()\n", 
        "    #fit\n", 
        "    est.fit(Xtrain, ytrain)\n", 
        "    #predict\n", 
        "    prediction_on_training = est.predict(Xtrain)\n", 
        "    prediction_on_test = est.predict(Xtest)\n", 
        "    #calculate mean squared error\n", 
        "    error_train[d] = mean_squared_error(ytrain, prediction_on_training)\n", 
        "    error_test[d] = mean_squared_error(ytest, prediction_on_test)\n", 
        "\n", 
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n", 
        "plt.plot(degrees, error_test, marker='o', label='test')\n", 
        "plt.axvline(np.argmin(error_test), 0,0.5, color='r', label=\"min test error at d=%d\"%np.argmin(error_test), alpha=0.3)\n", 
        "plt.ylabel('mean squared error')\n", 
        "plt.xlabel('degree')\n", 
        "plt.legend(loc='upper left')\n", 
        "plt.yscale(\"log\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The graph shows a very interesting structure. The training error decreases with increasing degree of the polynomial. This ought to make sense given what you know now: one can construct an arbitrarily complex polynomial to fit all the training data: indeed one could construct an order 24 polynomial which perfectly interpolates the 24 data points in the training set. You also know that this would do very badly on the test set as it would wiggle like mad to capture all the data points. And this is indeed what we see in the test set error. \n", 
        "\n", 
        "For extremely low degree polynomials like $d=0$ a flat line capturing the mean value of the data or $d=1$ a straight line fitting the data, the polynomial is not curvy enough to capturve the conbtours of the data. We are in the bias/deterministic error regime, where we will always have some difference between the data and the fit since the hypothesis is too simple. But, for degrees higher than 4, the polynomial starts to wiggle too much to capture the training data. The test set error increases as the predictive power of the polynomial goes down thanks to the contortions it must endure to fit the training data.\n", 
        "\n", 
        "Thus the test set error first decreases as the model get more expressive, and then, once we exceed a certain level of complexity (here indexed by $d$), it increases. This idea can be used to identify just the right amount of complexity in the model by picking as **the best hypothesis as the one that minimizes test set error** or risk. In our case this happens at $d=4$. (This exact number will depend on the random points chosen into the training and test sets) For complexity lower than this critical value, identified by the red vertical line in the diagram, the hypotheses underfit; for complexity higher, they overfit.\n", 
        "\n", 
        "![m:caption](images/complexity-error-plot.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "There is a problem with the process above which is not apparent at first look. What we have done in picking $d=4$ as the best hypothesis is that we have used the test set as a training set. How?\n", 
        "\n", 
        "Our process used the training set to fit for the **parameters**(values of the coefficients) of the polynomial of given degree $d$ based on minimizing the traing set error (empirical risk minimization). We then calculated the error on the test set at that $d$. If we go further and choose the best $d$ based on minimizing the test set error, we have then \"fit for\" $d$ on the test set. We will thus call $d$ a **hyperparameter** of the model.\n", 
        "\n", 
        "In this case, the test-set error will underestimate the true out-of-sample error (for a proof of this see Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. AMLBook, 2012.). Furthermore, we have **contaminated the test set** by fitting for $d$ on it; it is no longer a true test set.\n", 
        "\n", 
        "Thus, we must introduce a new **validation set** on which the complexity parameter $d$ is fit, and leave out a test set which we can use to estimate the true out-of-sample performance of our learner. The place of this set in the scheme of things is shown below:\n", 
        "\n", 
        "![m:caption](images/train-validate-test.png)\n", 
        "\n", 
        "We have split the old training set into a training set and a validation set, holding the old test aside for FINAL testing AFTER we have \"fit\" for complexity $d$. Obviously we have decreased the size of the data available for training further, but this is a price we must pay for obtaining a good estimate of the out-of-sample risk $\\cal{E}_{out}$ (also denoted as risk $R_{out}$) through the test risk $\\cal{E}_{test}$ ($R_{test}$).\n", 
        "\n", 
        "![m:caption](images/train-validate-test-cont.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The validation process is illustrated in these two figures. We first loop ober all the hypothesis sets that we wish to consider: in our case this is a loop over the complexity parameter $d$, the degree of the polynomials we will try and fit. Then for each degree $d$, we obtain a best fit model $g^-_d$ where the \"minus\" superscript indicates that we fit our model on the new training set which is obtained by removing (\"minusing\") a validation chunk (often the same size as the test chunk) from the old training set. We then \"test\" this model on the validation chunk, obtaining the validation error for the best-fit polynomial coefficients and for degree $d$. We move on to the next degree $d$ and repeat the process, just like before. We compare all the validation set errors, just like we did with the test errors earlier, and pick the degree $d_*$ which minimizes this validation set error.\n", 
        "\n", 
        "![caption](images/train-validate-test3.png)\n", 
        "\n", 
        "Having picked the hyperparameter $d_*$, we retrain using the hypothesis set $\\cal{H}_{*}$ on the entire old training-set to find the parameters of the polynomial of order $d_*$ and the corresponding best fit hypothesis $g_*$. Note that we left the minus off the $g$ to indicate that it was trained on the entire old traing set. We now compute the test error on the test set as an estimate of the test risk $\\cal{E}_{test}$.\n", 
        "\n", 
        "Thus the **validation** set if the set on which the hyperparameter is fit. This method of splitting the data $\\cal{D}$ is called the **train-validate-test** split.\n", 
        "\n", 
        "We carry out this process for one training/validation split below. Note the smaller size of the new training set. We hold the test set at the same size."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 43, 
      "cell_type": "code", 
      "source": [
        "#we split the training set down further\n", 
        "intrain,invalid = train_test_split(itrain,train_size=18, test_size=6)\n", 
        "xntrain= df.x[intrain].values\n", 
        "fntrain = df.f[intrain].values\n", 
        "yntrain = df.y[intrain].values\n", 
        "xnvalid= df.x[invalid].values\n", 
        "fnvalid = df.f[invalid].values\n", 
        "ynvalid = df.y[invalid].values\n", 
        "\n", 
        "degrees=range(21)\n", 
        "error_train=np.empty(len(degrees))\n", 
        "error_valid=np.empty(len(degrees))\n", 
        "trainvalidlists=make_features(xntrain, xnvalid, degrees)\n", 
        "\n", 
        "for d in degrees:#for increasing polynomial degrees 0,1,2...\n", 
        "    #Create polynomials from x\n", 
        "    Xntrain = trainvalidlists[d]['train']\n", 
        "    Xnvalid = trainvalidlists[d]['test']\n", 
        "    #fit a model linear in polynomial coefficients on the new smaller training set\n", 
        "    est = LinearRegression()\n", 
        "    est.fit(Xntrain, yntrain)\n", 
        "    #predict on new training and validation sets and calculate mean squared error\n", 
        "    error_train[d] = mean_squared_error(yntrain, est.predict(Xntrain))\n", 
        "    error_valid[d] = mean_squared_error(ynvalid, est.predict(Xnvalid))\n", 
        "\n", 
        "#calculate the degree at which validation error is minimized\n", 
        "mindeg = np.argmin(error_valid) \n", 
        "ttlist=make_features(xtrain, xtest, degrees)\n", 
        "#fit on whole training set now.\n", 
        "clf = LinearRegression()\n", 
        "clf.fit(ttlist[mindeg]['train'], ytrain) # fit\n", 
        "#predict on the test set now and calculate error\n", 
        "pred = clf.predict(ttlist[mindeg]['test'])\n", 
        "err = mean_squared_error(ytest, pred)\n", 
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n", 
        "plt.plot(degrees, error_valid, marker='o', label='validation')\n", 
        "plt.plot([mindeg], [err], marker='s', markersize=10, label='test', alpha=0.5, color='r')\n", 
        "plt.ylabel('mean squared error')\n", 
        "plt.xlabel('degree')\n", 
        "plt.legend(loc='upper left')\n", 
        "plt.yscale(\"log\")\n", 
        "mindeg"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We obtained a validation set minimizing polynomial degree of 4.\n", 
        "\n", 
        "Lets do this again, choosing a new random split between training and validation data: "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 44, 
      "cell_type": "code", 
      "source": [
        "intrain,invalid = train_test_split(itrain,train_size=18, test_size=6)\n", 
        "xntrain= df.x[intrain].values\n", 
        "fntrain = df.f[intrain].values\n", 
        "yntrain = df.y[intrain].values\n", 
        "xnvalid= df.x[invalid].values\n", 
        "fnvalid = df.f[invalid].values\n", 
        "ynvalid = df.y[invalid].values\n", 
        "\n", 
        "degrees=range(21)\n", 
        "error_train=np.empty(len(degrees))\n", 
        "error_valid=np.empty(len(degrees))\n", 
        "trainvalidlists=make_features(xntrain, xnvalid, degrees)\n", 
        "\n", 
        "for d in degrees:#for increasing polynomial degrees 0,1,2...\n", 
        "    #Create polynomials from x\n", 
        "    Xntrain = trainvalidlists[d]['train']\n", 
        "    Xnvalid = trainvalidlists[d]['test']\n", 
        "    #fit a model linear in polynomial coefficients on the training set\n", 
        "    est = LinearRegression()\n", 
        "    est.fit(Xntrain, yntrain)\n", 
        "    #calculate mean squared error\n", 
        "    error_train[d] = mean_squared_error(yntrain, est.predict(Xntrain))\n", 
        "    error_valid[d] = mean_squared_error(ynvalid, est.predict(Xnvalid))\n", 
        "\n", 
        "mindeg = np.argmin(error_valid)\n", 
        "ttlist=make_features(xtrain, xtest, degrees)\n", 
        "#fit on whole training set now.\n", 
        "clf = LinearRegression()\n", 
        "clf.fit(ttlist[mindeg]['train'], ytrain) # fit\n", 
        "pred = clf.predict(ttlist[mindeg]['test'])\n", 
        "err = mean_squared_error(ytest, pred)\n", 
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n", 
        "plt.plot(degrees, error_valid, marker='o', label='validation')\n", 
        "plt.plot([mindeg], [err], marker='s', markersize=10, label='test', alpha=0.5, color='r')\n", 
        "\n", 
        "plt.ylabel('mean squared error')\n", 
        "plt.xlabel('degree')\n", 
        "plt.legend(loc='lower left')\n", 
        "plt.yscale(\"log\")\n", 
        "mindeg"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "This time the validation error minimizing polynomial degree changed! What happened?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Cross Validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The Idea"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Remember that we chose a new random split? Since we are dealing with small data sizes here, you should worry that this exposes us to the peciliarity of the data set that got randomly chosen for us. This naturally leads us to want to choose multiple such random splits and somehow average over this process to find the \"best\" validation minimizing polynomial degree or complexity $d$.\n", 
        "\n", 
        "Furthermore the validation set that we left out has two competing demands on it. The larger the set is, the better is our estimate of the out-of-sample error. So we'd like to hold out as much as possible.\n", 
        "\n", 
        "But the smaller the validation set is, the more data we have to train ourmodel on. Thus we can fit a better, more expressive model.\n", 
        "\n", 
        "We want to balance these two desires, and additionally, not be exposed to any peculiarities that might randomly arise in any single train-validate split of the old training set.\n", 
        "\n", 
        "To deal with this we engage in a process called **cross-validation**, which is illustrated in the figure below, for a given hypothesis set $\\cal{H}_a$ with complexity parameter $d=a$ (the polynomial degree). We do the train/validate split, not once but multiple times. \n", 
        "\n", 
        "In the figure below we create 4-folds from the training set part of our data set $\\cal{D}$. By this we mean that we divide our set roughly into 4 equal parts. As illustrated below, this can be done in 4 different ways, or folds. In each fold we train a model on 3 of the parts. The model so trained is denotet as $g^-_{Fi}$, for example $g^-_{F3}$ . The minus sign in the superscript once again indicates that we are training on a reduced set. The $F3$ indicates that this model was trained on the third fold. Note that the model trained on each fold will be different!\n", 
        "\n", 
        "For each fold, after training the model, we calculate the risk or error on the remaining one validation part. We then add the validation errors together from the different folds, and divide by the number of folds to calculate an average error. Note again that this average error is an average over different models $g^-_{Fi}$. We use this error as the validation error for $d=a$ in the validation process described earlier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "![m:caption](images/train-cv2.png)\n", 
        "\n", 
        "Note that the number of folds is equal to the number of splits in the data. For example, if we have 5 splits, there will be 5 folds. To illustrate cross-validation consider below fits in $\\cal{H}_0$ and $\\cal{H}_1$ (means and straight lines) to a sine curve, with only 3 data points."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The entire description of K-fold Cross-validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We put thogether this scheme to calculate the error for a given polynomial degree $d$ with the method we used earlier to choose a model given the validation-set risk as a function of $d$:\n", 
        "\n", 
        "1. create `n_folds` partitions of the training data. \n", 
        "2. We then train on `n_folds -1` of these partitions, and test on the remaining partition. There are `n_folds` such combinations of partitions (or folds), and thus we obtain `n_fold` risks.\n", 
        "3. We average the error or risk of all such combinations to obtain, for each value of $d$, $R_{dCV}$.\n", 
        "4. We move on to the next value of $d$, and repeat 3\n", 
        "5. and then find the optimal value of d that minimizes risk $d=*$.\n", 
        "5. We finally use that value to make the final fit in $\\cal{H}_*$ on the entire old training set.\n", 
        "\n", 
        "![caption](images/train-cv3.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "It can also shown that **cross-validation error is an unbiased estimate of the out of sample-error**.\n", 
        "\n", 
        "Let us now do 4-fold cross-validation on our Romney votes data set. We increase the complexity from degree 0 to degree 20. In each case we take the old training set, split in 4 ways into 4 folds, train on 3 folds, and calculate the validation error on the ramining one. We then average the erros over the four folds to get a cross-validation error for that $d$. Then we did what we did before: find the hypothesis space $\\cal{H}_*$ with the lowest cross-validation error, and refit it using the entire training set. We can then use the test set to estimate $E_{out}$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 45, 
      "cell_type": "code", 
      "source": [
        "from sklearn.cross_validation import KFold\n", 
        "n_folds=4\n", 
        "degrees=range(21)\n", 
        "results=[]\n", 
        "for d in degrees:\n", 
        "    hypothesisresults=[]\n", 
        "    for train, test in KFold(24, n_folds): # split data into train/test groups, 4 times\n", 
        "        tvlist=make_features(xtrain[train], xtrain[test], degrees)\n", 
        "        clf = LinearRegression()\n", 
        "        clf.fit(tvlist[d]['train'], ytrain[train]) # fit\n", 
        "        hypothesisresults.append(mean_squared_error(ytrain[test], clf.predict(tvlist[d]['test']))) # evaluate score function on held-out data\n", 
        "    results.append((np.mean(hypothesisresults), np.min(hypothesisresults), np.max(hypothesisresults))) # average"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 46, 
      "cell_type": "code", 
      "source": [
        "mindeg = np.argmin([r[0] for r in results])\n", 
        "ttlist=make_features(xtrain, xtest, degrees)\n", 
        "#fit on whole training set now.\n", 
        "clf = LinearRegression()\n", 
        "clf.fit(ttlist[mindeg]['train'], ytrain) # fit\n", 
        "pred = clf.predict(ttlist[mindeg]['test'])\n", 
        "err = mean_squared_error(pred, ytest)\n", 
        "errtr=mean_squared_error(ytrain, clf.predict(ttlist[mindeg]['train']))\n", 
        "errout=0.8*errtr+0.2*err\n", 
        "c0=sns.color_palette()[0]\n", 
        "#plt.errorbar(degrees, [r[0] for r in results], yerr=[r[1] for r in results], marker='o', label='CV error', alpha=0.5)\n", 
        "plt.plot(degrees, [r[0] for r in results], marker='o', label='CV error', alpha=0.9)\n", 
        "plt.fill_between(degrees, [r[1] for r in results], [r[2] for r in results], color=c0, alpha=0.2)\n", 
        "plt.plot([mindeg], [err], 'o',  label='test set error')\n", 
        "plt.plot([mindeg], [errout], 'o',  label='full sample error')\n", 
        "\n", 
        "\n", 
        "plt.ylabel('mean squared error')\n", 
        "plt.xlabel('degree')\n", 
        "plt.legend(loc='upper right')\n", 
        "plt.yscale(\"log\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We see that the cross-validation error minimizes at a low degree, and then increases. Because we have so few data points the spread in fold errors increases as well."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Regularization"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Upto now we have focussed on finding the polynomial with the right degree of complecity $d=*$ given the data that we have.\n", 
        "\n", 
        "Let us now ask a different question: if we are going to fit the data with an expressive model such as 20th order polynomials, how can we **regularize** or smooth or restrict the choices of the kinds of 20th order polynomials that we allow in our fits. In other words, we are again trying to bring down the complexity of the hypothesis space, but by a different tack: a tack which prefers smooth polynomials over wiggly ones.\n", 
        "\n", 
        "That is, if we want to fit with a 20th order polynomial, ok, lets fit with it, but lets reduce the size of, or limit the functions in $\\cal{H}_{20}$ that we allow.\n", 
        "\n", 
        "In a sense we have already done this, havent we? When we compared $\\cal{H}_1$ over $\\cal{H}_{10}$, we imposed a **hard constraint** by setting all polynomial co-efficients for terms higher than $x$ to 0. Instead we might want to set a **soft constraint** by setting:\n", 
        "\n", 
        "$$\\sum_{i=0}^j a_i^2 < C.$$\n", 
        "\n", 
        "We can do this by adding a term to the empirical risk that we minimize on the training data for $\\cal{H}_j$ (seeing why is beyond the scope of this lab but google on lagrange multipliers and the dual problem):\n", 
        "\n", 
        "$$\\cal{R}(h_j) =  \\sum_{y_i \\in \\cal{D}} (y_i - h_j(x_i))^2 +\\alpha \\sum_{i=0}^j a_i^2.$$\n", 
        "\n", 
        "This new risk takes the empirical risk and adds a \"penalty term\" to it to minimize overfitting. The term is proportional to the sum of the squares of the coefficients and is positive, so it will keep their values down\n", 
        "\n", 
        "Notice that we are adding a term to the **training error**, once $\\alpha$ is defined. The entire structure is similar to what we did to find the optimal $d=*$, with $\\alpha$ being the analog of $d$. And thus we can use the same validation and cross-validation technology that we developed to estimate $d$.\n", 
        "\n", 
        "This technique is called **regularization** or **shrinkage** as it takes the coefficients $a_i$ towards smaller sizes. As you have seen earlier, for polynomials this corresponds to choosing smooth functions over wiggly functions. When $\\alpha=0$ we have the regular polynomial regression problem, and if we are using 20th order polynomials we will wildly overfit. We are in the high variance zone. The problem with a non-zero $\\alpha$ is called **ridge regression**. As $\\alpha$ increases, the importance of the penalty term increases at the expense of the ERM term, and we are pushed to increase the smoothness of the polynomials. When $\\alpha$ becomes very large the penalty term dominates and we get into the high bias zone. Thus $\\alpha$ acts as a complexity parameter just like $d$ did, with high complexity being $\\alpha \\to 0$.\n", 
        "\n", 
        "#### An illustration"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 47, 
      "cell_type": "code", 
      "source": [
        "xs=np.arange(-1.,1.,0.01)\n", 
        "ff = lambda x: np.sin(np.pi*x)\n", 
        "ffxs=ff(xs)\n", 
        "from sklearn.linear_model import Ridge\n", 
        "axes=make_simple_plot()\n", 
        "c0=sns.color_palette()[0]\n", 
        "c1=sns.color_palette()[1]\n", 
        "axes[0].plot(xs, ff(xs), alpha=0.9, lw=3, color=c0)\n", 
        "axes[1].plot(xs, ff(xs), alpha=0.9, lw=3, color=c0)\n", 
        "from sklearn.linear_model import Ridge\n", 
        "D=np.empty((100,3), dtype=\"int\")\n", 
        "print(D.shape)\n", 
        "for i in range(100):\n", 
        "    D[i,:] = np.random.choice(200, replace=False, size=3)\n", 
        "for i in range(100):\n", 
        "    choices = D[i,:]\n", 
        "    p1=np.polyfit(xs[choices], ffxs[choices],1)\n", 
        "    est = Ridge(alpha=1.0)\n", 
        "    est.fit(xs[choices].reshape(-1,1), ffxs[choices])\n", 
        "    axes[0].plot(xs, np.polyval(p1, xs), color=c1, alpha=0.2)\n", 
        "    axes[1].plot(xs, est.predict(xs.reshape(-1,1)), color=c1, alpha=0.2)\n", 
        "axes[0].set_title(\"Unregularized\");\n", 
        "axes[1].set_title(\"Regularized with $\\\\alpha=1.0$\");"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "In the left panel we plot unregularized straight line fits. The plot is hairy, since choosing 3 points from 200 between -1 and 1 dosent constrain the lines much at all. On the right panel, we plot the output of **Ridge** regression with $\\alpha=1$. This corresponds to adding a term to the empirical risk of $\\alpha\\, (a_0^2 + a_1^2)$ where $a_0$ and $a_1$ are the intercept and slope of the line respectively. Notice that the lines are much more constrained  in this second plot. The penalty term has regularized the values of the intercept and slope, and forced the intercept to be closer to 0 and the lines to be flatter."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Regularization of the Romney model with Cross-Validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 48, 
      "cell_type": "code", 
      "source": [
        "def plot_functions(est, ax, df, alpha, xtest, Xtest, xtrain, ytrain):\n", 
        "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n", 
        "    ax.plot(df.x, df.f, color='k', label='f')\n", 
        "    ax.plot(xtrain, ytrain, 's', label=\"training\", alpha=0.4)\n", 
        "    ax.plot(xtest, ytest, 's', label=\"testing\", alpha=0.6)\n", 
        "    transx=np.arange(0,1.1,0.01)\n", 
        "    transX = PolynomialFeatures(20).fit_transform(transx.reshape(-1,1))\n", 
        "    ax.plot(transx, est.predict(transX),  '.', label=\"alpha = %s\" % str(alpha))\n", 
        "    ax.set_ylim((0, 1))\n", 
        "    ax.set_xlim((0, 1))\n", 
        "    ax.set_ylabel('y')\n", 
        "    ax.set_xlabel('x')\n", 
        "    ax.legend(loc='lower right')\n", 
        "    \n", 
        "def plot_coefficients(est, ax, alpha):\n", 
        "    coef = est.coef_.ravel()\n", 
        "    ax.semilogy(np.abs(coef), marker='o', label=\"alpha = %s\" % str(alpha))\n", 
        "    ax.set_ylim((1e-1, 1e15))\n", 
        "    ax.set_ylabel('abs(coefficient)')\n", 
        "    ax.set_xlabel('coefficients')\n", 
        "    ax.legend(loc='upper left')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Lets now go back to the Romney voting model and see what regularization does to the fits in that model. The addition of a penalty term to the risk or error causes us to choose a smaller subset of the entire set of complex $\\cal{H}_{20}$ polynomials. This is shown in the diagram below where the balance between bias and variance occurs at some subset $S_*$ of the set of 20th order polynomials indexed by $\\alpha_*$ (there is an error on the diagram, the 13 there should actually be a 20).\n", 
        "\n", 
        "![m:caption](images/complexity-error-reg.png)\n", 
        "\n", 
        "Lets see what some of the $\\alpha$s do. The diagram below trains on the entire training set, for given values of $\\alpha$, minimizing the penalty-term-added training error."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 49, 
      "cell_type": "code", 
      "source": [
        "fig, rows = plt.subplots(4, 2, figsize=(12, 16))\n", 
        "d=20\n", 
        "alphas = [0.0, 1e-5, 1e-3, 1]\n", 
        "Xtrain = traintestlists[d]['train']\n", 
        "Xtest = traintestlists[d]['test']\n", 
        "for i, alpha in enumerate(alphas):\n", 
        "    l,r=rows[i]\n", 
        "    est = Ridge(alpha=alpha)\n", 
        "    est.fit(Xtrain, ytrain)\n", 
        "    plot_functions(est, l, df, alpha, xtest, Xtest, xtrain, ytrain )\n", 
        "    plot_coefficients(est, r, alpha)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "As you can see, as we increase $\\alpha$ from 0 to 1, we start out overfitting, then doing well, and then, our fits, develop a mind of their own irrespective of data, as the penalty term dominates the risk.\n", 
        "\n", 
        "Lets use cross-validation to figure what this critical $\\alpha_*$ is. To do this we use the concept of a *meta-estimator* from scikit-learn. As the API paper puts it:\n", 
        "\n", 
        ">In scikit-learn, model selection is supported in two distinct meta-estimators, GridSearchCV and RandomizedSearchCV. They take as input an estimator (basic or composite), whose hyper-parameters must be optimized, and a set of hyperparameter settings to search through.\n", 
        "\n", 
        "The concept of a meta-estimator allows us to wrap, for example, cross-validation, or methods that build and combine simpler models or schemes. For example:\n", 
        "\n", 
        "    clf = Ridge()\n", 
        "    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n", 
        "    gridclassifier=GridSearchCV(clf, param_grid=parameters, cv=4, scoring=\"mean_squared_error\")\n", 
        "    \n", 
        "The `GridSearchCV` replaces the manual iteration over thefolds using `KFolds` and the averaging we did previously, doint it all for us. It takes a parameter grid in the shape of a dictionary as input, and sets $\\alpha$ to the appropriate parameter values one by one. It then trains the model, cross-validation fashion, and gets the error. Finally it compares the errors for the different $\\alpha$'s, and picks the best choice model."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 50, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import make_scorer\n", 
        "#, 1e-6, 1e-5, 1e-3, 1.0\n", 
        "from sklearn.grid_search import GridSearchCV\n", 
        "def cv_optimize_ridge(X, y, n_folds=4):\n", 
        "    clf = Ridge()\n", 
        "    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n", 
        "    #the scoring parameter below is the default one in ridge, but you can use a different one\n", 
        "    #in the cross-validation phase if you want.\n", 
        "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=\"mean_squared_error\")\n", 
        "    gs.fit(X, y)\n", 
        "    return gs"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 51, 
      "cell_type": "code", 
      "source": [
        "fitmodel = cv_optimize_ridge(Xtrain, ytrain, n_folds=4)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 52, 
      "cell_type": "code", 
      "source": [
        "fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Our best model occurs for $\\alpha=0.01$. We also output the mean cross-validation error at different $\\alpha$ (with a negative sign, as scikit-learn likes to maximize negative error which is equivalent to minimizing error).\n", 
        "\n", 
        "We refit the estimator on old training set, and calculate and plot the test set error and the polynomial coefficients. Notice how many of these coefficients have been pushed to lower values or 0."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 53, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 54, 
      "cell_type": "code", 
      "source": [
        "err=mean_squared_error(clf.predict(Xtest), ytest)\n", 
        "errtr=mean_squared_error(clf.predict(Xtrain), ytrain)\n", 
        "errout=0.8*errtr+0.2*err"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 55, 
      "cell_type": "code", 
      "source": [
        "def plot_functions_onall(est, ax, df, alpha, xtrain, ytrain, Xtrain, xtest, ytest):\n", 
        "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n", 
        "    ax.plot(df.x, df.f, color='k', label='f')\n", 
        "    ax.plot(xtrain, ytrain, 's', alpha=0.4, label=\"train\")\n", 
        "    ax.plot(xtest, ytest, 's', alpha=0.6, label=\"test\")\n", 
        "    transx=np.arange(0,1.1,0.01)\n", 
        "    transX = PolynomialFeatures(20).fit_transform(transx.reshape(-1,1))\n", 
        "    ax.plot(transx, est.predict(transX), '.', alpha=0.6, label=\"alpha = %s\" % str(alpha))\n", 
        "    #print est.predict(transX)\n", 
        "    ax.set_ylim((0, 1))\n", 
        "    ax.set_xlim((0, 1))\n", 
        "    ax.set_ylabel('y')\n", 
        "    ax.set_xlabel('x')\n", 
        "    ax.legend(loc='lower right')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 56, 
      "cell_type": "code", 
      "source": [
        "fig, rows = plt.subplots(1, 2, figsize=(12, 5))\n", 
        "l,r=rows\n", 
        "plot_functions_onall(clf, l, df, alphawechoose, xtrain, ytrain, Xtrain, xtest, ytest)\n", 
        "plot_coefficients(clf, r, alphawechoose)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "As we can see, the best fit model is now chosen from the entire set of 20th order polynomials, and a non-zero hyperparameter $\\alpha$ that we fit for ensures that only smooth models amonst these polynomials are chosen, by setting most of the polynomial coefficients to something close to 0 (Lasson sets them exactly to 0)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### What is minimized where?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Regularization adds a term to the error or risk, which is then, for fixed $\\alpha$, minimized on the training set.\n", 
        "The validation error is then calculated on the validation set and reported alongside the value of $\\alpha$ used. If we cross-validate, then the validation error is an average over multiple folds.\n", 
        "\n", 
        "On the other hand, when we validate for $d$, we use the validation error as an estimate of the out-of-sample error and use this estimate to directly pick the best model (best $d$).\n", 
        "\n", 
        "One can think of **regularization as an estimation of the overfit term** while **validation directly estimates $R_{out}$**."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }
  ]
}